{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Reinforcement Learning Algorithms\n",
    "\n",
    "## Sample Code p. 25\n",
    "\n",
    "### Notes regarding the objects used below\n",
    "\n",
    "1. keras.layers.Input(): Calling .Input() function will instantiate a symbolic Keras tensor (i.e. a placeholder). Specifying a shape tuple indicates the expected input, while None refers to an unknow shape. \n",
    "2. keras.layers.Dense()(): First set of parentheses specifies the layer attributes, the second is where the prior layers' output goes. Alternatively the layers can be arranged by using tf.keras.models.Sequential() whicht takes care of what serves as input/output to what and in wht shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load neural_networks/policy_gradient_utilities.py\n",
    "#!/usr/bin/env python2\n",
    "\"\"\"\n",
    "Created on Mon Mar 25 15:22:27 2019\n",
    "\n",
    "@author: tawehbeysolow\n",
    "@notes: gianrothfuchs\n",
    "\"\"\"\n",
    "\n",
    "import keras.layers as layers\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "class PolicyGradient():\n",
    "    \n",
    "    def __init__(self, n_units, n_layers, n_columns, n_outputs, learning_rate, hidden_activation, output_activation, loss_function):\n",
    "        self.n_units = n_units\n",
    "        self.n_layers = n_layers\n",
    "        self.n_columns = n_columns\n",
    "        self.n_outputs = n_outputs\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "    def methodlog_likelihood_loss(actual_labels, predicted_labels):\n",
    "        log_likelihood = backend.log(actual_labels * (actual_labels - predicted_labels) + \n",
    "                              (1 - actual_labels) * (actual_labels + predicted_labels))\n",
    "        return backend.mean(log_likelihood * advantages, keepdims=True)\n",
    "\n",
    "    def create_policy_model(self, input_shape):\n",
    "        input_layer = layers.Input(shape=input_shape)\n",
    "        advantages = layers.Input(shape=[1])\n",
    "        \n",
    "        hidden_layer = layers.Dense(units=self.n_units, \n",
    "                                    activation=self.hidden_activation,\n",
    "                                    use_bias=False,\n",
    "                                    kernel_initializer=glorot_uniform(seed=42))(input_layer)\n",
    "        \n",
    "        output_layer = layers.Dense(units=self.n_outputs, \n",
    "                                    activation=self.output_activation,\n",
    "                                    use_bias=False,\n",
    "                                    kernel_initializer=glorot_uniform(seed=42))(hidden_layer)\n",
    "        \n",
    "\n",
    "        \n",
    "        if self.loss_function == 'log_likelihood':\n",
    "            self.loss_function = self.methodlog_likelihood_loss\n",
    "        else:\n",
    "            self.loss_function = 'categorical_crossentropy'\n",
    "                \n",
    "        policy_model = Model(inputs=[input_layer, advantages], outputs=output_layer)\n",
    "        policy_model.compile(loss=self.loss_function, optimizer=Adam(self.learning_rate))\n",
    "        model_prediction = Model(input=[input_layer], outputs=output_layer)\n",
    "        return policy_model, model_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/gian/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gian/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3313: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/gian/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gian/Dropbox/programming/applied_RL/applied-reinforcement-learning-w-python/neural_networks/policy_gradient_utilities.py:53: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "  model_prediction = Model(input=[input_layer], outputs=output_layer)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# %load chapter2/cart_pole_example.py\n",
    "#!/usr/bin/env python2\n",
    "\"\"\"\n",
    "Created on Wed Feb 20 13:50:58 2019\n",
    "\n",
    "@author: tawehbeysolow\n",
    "\"\"\"\n",
    "import keras\n",
    "\n",
    "\n",
    "import gym, numpy as np, matplotlib.pyplot as plt\n",
    "from neural_networks.policy_gradient_utilities import PolicyGradient\n",
    "\n",
    "#Parameters \n",
    "n_units = 5\n",
    "gamma = .99\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "n_episodes = 500\n",
    "render = False\n",
    "goal = 195\n",
    "n_layers = 2\n",
    "n_classes = 2\n",
    "environment = gym.make('CartPole-v1')\n",
    "environment_dimension = len(environment.reset())\n",
    "            \n",
    "def calculate_discounted_reward(reward, gamma=gamma):\n",
    "    output = [reward[i] * gamma**i for i in range(0, len(reward))]\n",
    "    return output[::-1]\n",
    "\n",
    "def score_model(model, n_tests, render=render):\n",
    "    scores = []    \n",
    "    for _ in range(n_tests):\n",
    "        environment.reset()\n",
    "        observation = environment.reset()\n",
    "        reward_sum = 0\n",
    "        while True:\n",
    "            if render:\n",
    "                environment.render()\n",
    "                \n",
    "            state = np.reshape(observation, [1, environment_dimension])\n",
    "            predict = model.predict([state])[0]\n",
    "            action = np.argmax(predict)\n",
    "            observation, reward, done, _ = environment.step(action)\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(reward_sum)\n",
    "        \n",
    "    environment.close()\n",
    "    return np.mean(scores)\n",
    "\n",
    "def cart_pole_game(environment, policy_model, model_predictions):\n",
    "    loss = []\n",
    "    n_episode, reward_sum, score, episode_done = 0, 0, 0, False\n",
    "    n_actions = environment.action_space.n\n",
    "    observation = environment.reset()\n",
    "    \n",
    "    states = np.empty(0).reshape(0, environment_dimension)\n",
    "    actions = np.empty(0).reshape(0, 1)\n",
    "    rewards = np.empty(0).reshape(0, 1)\n",
    "    discounted_rewards = np.empty(0).reshape(0, 1)\n",
    "    \n",
    "    while n_episode < n_episodes: \n",
    "         \n",
    "        state = np.reshape(observation, [1, environment_dimension])        \n",
    "        prediction = model_predictions.predict([state])[0]\n",
    "        action = np.random.choice(range(environment.action_space.n), p=prediction)\n",
    "        states = np.vstack([states, state])\n",
    "        actions = np.vstack([actions, action])\n",
    "        \n",
    "        observation, reward, episode_done, info = environment.step(action)\n",
    "        reward_sum += reward\n",
    "        rewards = np.vstack([rewards, reward])\n",
    "\n",
    "        if episode_done == True:\n",
    "            \n",
    "            discounted_reward = calculate_discounted_reward(rewards)\n",
    "            discounted_rewards = np.vstack([discounted_rewards, discounted_reward])\n",
    "            rewards = np.empty(0).reshape(0, 1)\n",
    "            \n",
    "            if (n_episode + 1) % batch_size == 0:\n",
    "                \n",
    "                discounted_rewards -= discounted_rewards.mean()\n",
    "                discounted_rewards /= discounted_rewards.std()\n",
    "                discounted_rewards = discounted_rewards.squeeze()\n",
    "                actions = actions.squeeze().astype(int)\n",
    "                \n",
    "                train_actions = np.zeros([len(actions), n_actions])\n",
    "                train_actions[np.arange(len(actions)), actions] = 1\n",
    "                \n",
    "                error = policy_model.train_on_batch([states, discounted_rewards], train_actions)\n",
    "                loss.append(error)\n",
    "                \n",
    "                states = np.empty(0).reshape(0, environment_dimension)\n",
    "                actions = np.empty(0).reshape(0, 1)\n",
    "                discounted_rewards = np.empty(0).reshape(0, 1)\n",
    "                                \n",
    "                #score = score_model(model=model_predictions, n_tests=100)\n",
    "                \n",
    "                #print('\\nEpisode: {} \\nAverage Reward: {}  \\nScore: {} \\nError: {}'.format(n_episode+1, reward_sum/float(batch_size), score, np.mean(loss[-batch_size:])))\n",
    "    \n",
    "                #if score >= goal:\n",
    "                #    break \n",
    "                \n",
    "                reward_sum = 0\n",
    "                \n",
    "            n_episode += 1\n",
    "            observation = environment.reset()\n",
    "            \n",
    "    plt.title('Policy Gradient Error plot over %s Episodes'%(n_episode+1))\n",
    "    plt.xlabel('N batches')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.plot(loss)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    \n",
    "    mlp_model = PolicyGradient(n_units=n_units, \n",
    "                              n_layers=n_layers, \n",
    "                              n_columns=environment_dimension, \n",
    "                              n_outputs=n_classes, \n",
    "                              learning_rate=learning_rate, \n",
    "                              hidden_activation='selu', \n",
    "                              output_activation='softmax',\n",
    "                              loss_function='log_likelihood')\n",
    "        \n",
    "    policy_model, model_predictions = mlp_model.create_policy_model(input_shape=(environment_dimension, ))\n",
    "    \n",
    "    policy_model.summary()\n",
    "    \n",
    "    cart_pole_game(environment=environment, \n",
    "                   policy_model=policy_model, \n",
    "                   model_predictions=model_predictions)\n",
    "    \n",
    "    model_predictions.save('savings/chapter2/cartpole/e50k.h5')\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whatch the Reinforcement-Learner Play\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/chapter2/cartpole/vid.mp4\",width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as layers\n",
    "\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "import gym, numpy as np, matplotlib.pyplot as plt\n",
    "from neural_networks.policy_gradient_utilities import PolicyGradient\n",
    "\n",
    "\n",
    "\n",
    "n_units = 5\n",
    "gamma = .99\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "n_episodes = 500\n",
    "render = False\n",
    "goal = 193\n",
    "n_layers = 2\n",
    "n_classes = 2\n",
    "environment = gym.make('CartPole-v1')\n",
    "environment_dimension = len(environment.reset())\n",
    "\n",
    "mlp_model = PolicyGradient(n_units=n_units, \n",
    "                          n_layers=n_layers, \n",
    "                          n_columns=environment_dimension, \n",
    "                          n_outputs=n_classes, \n",
    "                          learning_rate=learning_rate, \n",
    "                          hidden_activation='selu', \n",
    "                          output_activation='softmax',\n",
    "                          loss_function='log_likelihood')\n",
    "\n",
    "model_predictions = keras.models.load_model('savings/chapter2/cartpole/e50k.h5',compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VideoRec as vr\n",
    "video_recorder = None\n",
    "video_recorder = vr.VideoRecorder(\n",
    "        environment, \"videos/chapter2/cartpole/vid.mp4\", enabled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "\n",
    "n_episode, reward_sum, score, episode_done = 0, 0, 0, False\n",
    "n_actions = environment.action_space.n\n",
    "observation = environment.reset()\n",
    "\n",
    "\n",
    "states_log = np.empty(0).reshape(0, environment_dimension)\n",
    "actions_log = np.empty(0).reshape(0, 1)\n",
    "rewards_log = np.empty(0).reshape(0, 1)\n",
    "discounted_rewards = np.empty(0).reshape(0, 1)\n",
    "\n",
    "while not episode_done: \n",
    "    \n",
    "    #environment.render()\n",
    "    environment.unwrapped.render()\n",
    "    video_recorder.capture_frame()\n",
    "    state = np.reshape(observation, [1, environment_dimension])        \n",
    "    prediction = model_predictions.predict([state])[0]\n",
    "    action = np.random.choice(range(environment.action_space.n), p=prediction)\n",
    "    states_log = np.vstack([states_log, state])\n",
    "    actions_log = np.vstack([actions_log, action])\n",
    "\n",
    "    observation, reward, episode_done, info = environment.step(action)\n",
    "    reward_sum += reward\n",
    "    rewards_log = np.vstack([rewards_log, reward])\n",
    "    \n",
    "if video_recorder.enabled:\n",
    "    video_recorder.close()\n",
    "    video_recorder.enabled = False\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
