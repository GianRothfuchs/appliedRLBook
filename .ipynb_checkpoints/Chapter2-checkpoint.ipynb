{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Reinforcement Learning Algorithms\n",
    "\n",
    "## Sample Code p. 25\n",
    "\n",
    "### Notes regarding the objects used below\n",
    "\n",
    "1. keras.layers.Input(): Calling .Input() function will instantiate a symbolic Keras tensor (i.e. a placeholder). Specifying a shape tuple indicates the expected input, while None refers to an unknow shape. \n",
    "2. keras.layers.Dense()(): First set of parentheses specifies the layer attributes, the second is where the prior layers' output goes. Alternatively the layers can be arranged by using tf.keras.models.Sequential() whicht takes care of what serves as input/output to what and in wht shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %load neural_networks/policy_gradient_utilities.py\n",
    "#!/usr/bin/env python2\n",
    "\"\"\"\n",
    "Created on Mon Mar 25 15:22:27 2019\n",
    "\n",
    "@author: tawehbeysolow\n",
    "@notes: gianrothfuchs\n",
    "\"\"\"\n",
    "\n",
    "import keras.layers as layers\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "class PolicyGradient():\n",
    "    \n",
    "    def __init__(self, n_units, n_layers, n_columns, n_outputs, learning_rate, hidden_activation, output_activation, loss_function):\n",
    "        self.n_units = n_units\n",
    "        self.n_layers = n_layers\n",
    "        self.n_columns = n_columns\n",
    "        self.n_outputs = n_outputs\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "    def methodlog_likelihood_loss(actual_labels, predicted_labels):\n",
    "        log_likelihood = backend.log(actual_labels * (actual_labels - predicted_labels) + \n",
    "                              (1 - actual_labels) * (actual_labels + predicted_labels))\n",
    "        return backend.mean(log_likelihood * advantages, keepdims=True)\n",
    "\n",
    "    def create_policy_model(self, input_shape):\n",
    "        input_layer = layers.Input(shape=input_shape)\n",
    "        advantages = layers.Input(shape=[1])\n",
    "        \n",
    "        hidden_layer = layers.Dense(units=self.n_units, \n",
    "                                    activation=self.hidden_activation,\n",
    "                                    use_bias=False,\n",
    "                                    kernel_initializer=glorot_uniform(seed=42))(input_layer)\n",
    "        \n",
    "        output_layer = layers.Dense(units=self.n_outputs, \n",
    "                                    activation=self.output_activation,\n",
    "                                    use_bias=False,\n",
    "                                    kernel_initializer=glorot_uniform(seed=42))(hidden_layer)\n",
    "        \n",
    "\n",
    "        \n",
    "        if self.loss_function == 'log_likelihood':\n",
    "            self.loss_function = self.methodlog_likelihood_loss\n",
    "        else:\n",
    "            self.loss_function = 'categorical_crossentropy'\n",
    "                \n",
    "        policy_model = Model(inputs=[input_layer, advantages], outputs=output_layer)\n",
    "        policy_model.compile(loss=self.loss_function, optimizer=Adam(self.learning_rate))\n",
    "        model_prediction = Model(input=[input_layer], outputs=output_layer)\n",
    "        return policy_model, model_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-539986b52517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneural_networks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_gradient_utilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolicyGradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "# %load chapter2/cart_pole_example.py\n",
    "#!/usr/bin/env python2\n",
    "\"\"\"\n",
    "Created on Wed Feb 20 13:50:58 2019\n",
    "\n",
    "@author: tawehbeysolow\n",
    "\"\"\"\n",
    "import keras\n",
    "\n",
    "\n",
    "import gym, numpy as np, matplotlib.pyplot as plt\n",
    "from neural_networks.policy_gradient_utilities import PolicyGradient\n",
    "\n",
    "#Parameters \n",
    "n_units = 5\n",
    "gamma = .99\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "n_episodes = 1000000\n",
    "render = False\n",
    "goal = 195\n",
    "n_layers = 2\n",
    "n_classes = 2\n",
    "environment = gym.make('CartPole-v1')\n",
    "environment_dimension = len(environment.reset())\n",
    "            \n",
    "def calculate_discounted_reward(reward, gamma=gamma):\n",
    "    output = [reward[i] * gamma**i for i in range(0, len(reward))]\n",
    "    return output[::-1]\n",
    "\n",
    "def score_model(model, n_tests, render=render):\n",
    "    scores = []    \n",
    "    for _ in range(n_tests):\n",
    "        environment.reset()\n",
    "        observation = environment.reset()\n",
    "        reward_sum = 0\n",
    "        while True:\n",
    "            if render:\n",
    "                environment.render()\n",
    "                \n",
    "            state = np.reshape(observation, [1, environment_dimension])\n",
    "            predict = model.predict([state])[0]\n",
    "            action = np.argmax(predict)\n",
    "            observation, reward, done, _ = environment.step(action)\n",
    "            reward_sum += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(reward_sum)\n",
    "        \n",
    "    environment.close()\n",
    "    return np.mean(scores)\n",
    "\n",
    "def cart_pole_game(environment, policy_model, model_predictions):\n",
    "    loss = []\n",
    "    n_episode, reward_sum, score, episode_done = 0, 0, 0, False\n",
    "    n_actions = environment.action_space.n\n",
    "    observation = environment.reset()\n",
    "    \n",
    "    states = np.empty(0).reshape(0, environment_dimension)\n",
    "    actions = np.empty(0).reshape(0, 1)\n",
    "    rewards = np.empty(0).reshape(0, 1)\n",
    "    discounted_rewards = np.empty(0).reshape(0, 1)\n",
    "    \n",
    "    while n_episode < n_episodes: \n",
    "         \n",
    "        state = np.reshape(observation, [1, environment_dimension])        \n",
    "        prediction = model_predictions.predict([state])[0]\n",
    "        action = np.random.choice(range(environment.action_space.n), p=prediction)\n",
    "        states = np.vstack([states, state])\n",
    "        actions = np.vstack([actions, action])\n",
    "        \n",
    "        observation, reward, episode_done, info = environment.step(action)\n",
    "        reward_sum += reward\n",
    "        rewards = np.vstack([rewards, reward])\n",
    "\n",
    "        if episode_done == True:\n",
    "            \n",
    "            discounted_reward = calculate_discounted_reward(rewards)\n",
    "            discounted_rewards = np.vstack([discounted_rewards, discounted_reward])\n",
    "            rewards = np.empty(0).reshape(0, 1)\n",
    "            \n",
    "            if (n_episode + 1) % batch_size == 0:\n",
    "                \n",
    "                discounted_rewards -= discounted_rewards.mean()\n",
    "                discounted_rewards /= discounted_rewards.std()\n",
    "                discounted_rewards = discounted_rewards.squeeze()\n",
    "                actions = actions.squeeze().astype(int)\n",
    "                \n",
    "                train_actions = np.zeros([len(actions), n_actions])\n",
    "                train_actions[np.arange(len(actions)), actions] = 1\n",
    "                \n",
    "                error = policy_model.train_on_batch([states, discounted_rewards], train_actions)\n",
    "                loss.append(error)\n",
    "                \n",
    "                states = np.empty(0).reshape(0, environment_dimension)\n",
    "                actions = np.empty(0).reshape(0, 1)\n",
    "                discounted_rewards = np.empty(0).reshape(0, 1)\n",
    "                                \n",
    "                #score = score_model(model=model_predictions, n_tests=100)\n",
    "                \n",
    "                #print('\\nEpisode: {} \\nAverage Reward: {}  \\nScore: {} \\nError: {}'.format(n_episode+1, reward_sum/float(batch_size), score, np.mean(loss[-batch_size:])))\n",
    "    \n",
    "                #if score >= goal:\n",
    "                #    break \n",
    "                \n",
    "                reward_sum = 0\n",
    "                \n",
    "            n_episode += 1\n",
    "            observation = environment.reset()\n",
    "            \n",
    "    plt.title('Policy Gradient Error plot over %s Episodes'%(n_episode+1))\n",
    "    plt.xlabel('N batches')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.plot(loss)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    \n",
    "    mlp_model = PolicyGradient(n_units=n_units, \n",
    "                              n_layers=n_layers, \n",
    "                              n_columns=environment_dimension, \n",
    "                              n_outputs=n_classes, \n",
    "                              learning_rate=learning_rate, \n",
    "                              hidden_activation='selu', \n",
    "                              output_activation='softmax',\n",
    "                              loss_function='log_likelihood')\n",
    "        \n",
    "    policy_model, model_predictions = mlp_model.create_policy_model(input_shape=(environment_dimension, ))\n",
    "    \n",
    "    policy_model.summary()\n",
    "    \n",
    "    cart_pole_game(environment=environment, \n",
    "                   policy_model=policy_model, \n",
    "                   model_predictions=model_predictions)\n",
    "    \n",
    "    model_predictions.save('savings/chapter2/cartpole/e50k.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whatch the Reinforcement-Learner Play\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/chapter2/cartpole/vid.mp4\" controls  width=\"600\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/chapter2/cartpole/vid.mp4\",width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as layers\n",
    "\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "import gym, numpy as np, matplotlib.pyplot as plt\n",
    "from neural_networks.policy_gradient_utilities import PolicyGradient\n",
    "\n",
    "\n",
    "\n",
    "n_units = 5\n",
    "gamma = .99\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "n_episodes = 500\n",
    "render = False\n",
    "goal = 193\n",
    "n_layers = 2\n",
    "n_classes = 2\n",
    "environment = gym.make('CartPole-v1')\n",
    "environment_dimension = len(environment.reset())\n",
    "\n",
    "mlp_model = PolicyGradient(n_units=n_units, \n",
    "                          n_layers=n_layers, \n",
    "                          n_columns=environment_dimension, \n",
    "                          n_outputs=n_classes, \n",
    "                          learning_rate=learning_rate, \n",
    "                          hidden_activation='selu', \n",
    "                          output_activation='softmax',\n",
    "                          loss_function='log_likelihood')\n",
    "\n",
    "model_predictions = keras.models.load_model('savings/chapter2/cartpole/e50k.h5',compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import VideoRec as vr\n",
    "video_recorder = None\n",
    "video_recorder = vr.VideoRecorder(\n",
    "        environment, \"videos/chapter2/cartpole/vid.mp4\", enabled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/gian/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "\n",
    "n_episode, reward_sum, score, episode_done = 0, 0, 0, False\n",
    "n_actions = environment.action_space.n\n",
    "observation = environment.reset()\n",
    "\n",
    "\n",
    "states_log = np.empty(0).reshape(0, environment_dimension)\n",
    "actions_log = np.empty(0).reshape(0, 1)\n",
    "rewards_log = np.empty(0).reshape(0, 1)\n",
    "discounted_rewards = np.empty(0).reshape(0, 1)\n",
    "\n",
    "while not episode_done: \n",
    "    \n",
    "    #environment.render()\n",
    "    environment.unwrapped.render()\n",
    "    video_recorder.capture_frame()\n",
    "    state = np.reshape(observation, [1, environment_dimension])        \n",
    "    prediction = model_predictions.predict([state])[0]\n",
    "    action = np.random.choice(range(environment.action_space.n), p=prediction)\n",
    "    states_log = np.vstack([states_log, state])\n",
    "    actions_log = np.vstack([actions_log, action])\n",
    "\n",
    "    observation, reward, episode_done, info = environment.step(action)\n",
    "    reward_sum += reward\n",
    "    rewards_log = np.vstack([rewards_log, reward])\n",
    "    \n",
    "if video_recorder.enabled:\n",
    "    video_recorder.close()\n",
    "    video_recorder.enabled = False\n",
    "\n",
    "environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
